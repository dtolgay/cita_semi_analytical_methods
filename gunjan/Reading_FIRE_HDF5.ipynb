{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8b6124-e454-4252-81a1-7fb85e148eb2",
   "metadata": {},
   "source": [
    "# Reading HDF5 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a207d-09eb-4574-a26d-a819938501fb",
   "metadata": {},
   "source": [
    "This notebook describes a little bit of basics about how to read hdf5 files in python using the library/module h5py.\n",
    "\n",
    "Before we start, we can understand a little about what HDF5 format is. \n",
    "\n",
    "From Wikipedia:\n",
    "\n",
    "**Hierarchical Data Format (HDF) is a set of file formats (HDF4, HDF5) designed to store and organize large amounts of data. Originally developed at the National Center for Supercomputing Applications, it is supported by The HDF Group, a non-profit corporation whose mission is to ensure continued development of HDF5 technologies and the continued accessibility of data stored in HDF.**\n",
    "\n",
    "HDF5 simplifies the file structure to include only two major types of object:\n",
    "\n",
    "1. Datasets, which are multidimensional arrays of a homogeneous type.\n",
    "\n",
    "2. Groups, which are container structures which can hold datasets and other groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f925ad7-1843-444d-ab1e-4074ce2feddf",
   "metadata": {},
   "source": [
    "### General Notes about FIRE HDF5 snapshot files:\n",
    "The snapshot files from the FIRE simulations are also stored in HDF5 format. Due to their large size, the files are usually broken down into smaller chunks, and a given snapshot may contain around 5-10 individual hdf5 files that should be combined to make a single dataset for a given snapshot. \n",
    "The files have some stucture in them. There are 4 different groups that are stored in the FIRE HDF5 files:\n",
    "\n",
    "1. **Header**: This is the metadata for the given snapshot subfile. It contains a lot of useful information, but mainly contains the number of particles in the snapshot totally and the number of particles in a given smaller individual chunk of the snapshot (when it is broken down into smaller files). This can be used to verify some diagnostics.\n",
    "\n",
    "2. **PartType0**: This is the gas particle group.\n",
    "3. **PartType1**: This is the dm particle group.\n",
    "4. **PartType2**: ....Dummy particles, not really used, as far as I know. \n",
    "5. **PartType3**: This is the stellar particle group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b753301-deea-43c8-a784-44896f8836f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pprint # this is the pretty print module in python, it prints dictionaries in nicer formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9cf6439-0cd5-4b64-af62-e2f2ab12afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will read an example snapshot to understand how to do this in general\n",
    "# for any hdf5 file\n",
    "test_file = h5py.File('../snapdir_440/snapshot_440.0.hdf5', 'r') # load the hdf5 file in read mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bbdbee65-29ab-449f-b9bf-dc5e2b185830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"snapshot_440.0.hdf5\" (mode r)>\n"
     ]
    }
   ],
   "source": [
    "print(test_file)\n",
    "# as you see, the output says nothing useful. This is because this is an\n",
    "# HDF5 file that needs some special working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52323a0d-a7d1-4eb3-97ba-4c11f9459544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in this HDF5 file are: <KeysViewHDF5 ['Header', 'PartType0', 'PartType1', 'PartType2', 'PartType4']>\n",
      "\n",
      "Values in this HDF5 file are: ValuesViewHDF5(<HDF5 file \"snapshot_440.0.hdf5\" (mode r)>)\n"
     ]
    }
   ],
   "source": [
    "# HDF5 files are organized at a high-level like a python dictionary\n",
    "# they have keys and values\n",
    "print(f'Keys in this HDF5 file are: {test_file.keys()}\\n')\n",
    "print(f'Values in this HDF5 file are: {test_file.values()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ed5ac81-c15a-40e1-af18-ca09468e4207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Header Keys are: <KeysViewHDF5 []>'\n",
      "'Header Values are: ValuesViewHDF5(<HDF5 group \"/Header\" (0 members)>)'\n"
     ]
    }
   ],
   "source": [
    "# as you see above, there is still not a lot of useful information, however, from the list of keys\n",
    "# we finally see that this file contains 5 different 'datasets'\n",
    "# lets go ahead and parse the Header dataset from the HDF5 file\n",
    "# Header in the snapshots is a special type of file that is responsible for\n",
    "# storing the metadata of the simulation snapshot. It contains very useful information.\n",
    "header_info = test_file['Header']\n",
    "pprint.pprint(f'Header Keys are: {header_info.keys()}')\n",
    "pprint.pprint(f'Header Values are: {header_info.values()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "511b9bc6-a58a-4ff8-82c0-a50c99de2c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attribute keys inside the Header Group is: <KeysViewHDF5 ['BoxSize', 'Flag_Cooling', 'Flag_DoublePrecision', 'Flag_Feedback', 'Flag_IC_Info', 'Flag_Metals', 'Flag_Sfr', 'Flag_StellarAge', 'HubbleParam', 'MassTable', 'NumFilesPerSnapshot', 'NumPart_ThisFile', 'NumPart_Total', 'NumPart_Total_HighWord', 'Omega0', 'OmegaLambda', 'Redshift', 'Time']>\n"
     ]
    }
   ],
   "source": [
    "# since the Header is a special group in the HDF5 file, we need to read it another way\n",
    "# we need to use .attrs method to look at all keys of the attributes that are defined \n",
    "# in the HDF5 Header group\n",
    "print(f'The attribute keys inside the Header Group is: {header_info.attrs.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9e80087-708f-4305-ae78-38f4b25d1ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BoxSize': 60000.0,\n",
      " 'Flag_Cooling': 1,\n",
      " 'Flag_DoublePrecision': 0,\n",
      " 'Flag_Feedback': 1,\n",
      " 'Flag_IC_Info': 3,\n",
      " 'Flag_Metals': 12,\n",
      " 'Flag_Sfr': 1,\n",
      " 'Flag_StellarAge': 1,\n",
      " 'HubbleParam': 0.702,\n",
      " 'MassTable': array([0.        , 0.00015806, 0.        , 0.        , 0.        ,\n",
      "       0.        ]),\n",
      " 'NumFilesPerSnapshot': 8,\n",
      " 'NumPart_ThisFile': array([1599950, 2353598,  224541,       0,  209181,       0], dtype=int32),\n",
      " 'NumPart_Total': array([ 8887776, 10403824,  4845138,        0,  1516048,        0],\n",
      "      dtype=uint32),\n",
      " 'NumPart_Total_HighWord': array([0, 0, 0, 0, 0, 0], dtype=uint32),\n",
      " 'Omega0': 0.272,\n",
      " 'OmegaLambda': 0.728,\n",
      " 'Redshift': 0.0,\n",
      " 'Time': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Now obtain each individual metadata attribute\n",
    "header_dict = {}\n",
    "for attribute in header_info.attrs.keys():\n",
    "    header_dict[attribute] = header_info.attrs[attribute]\n",
    "pprint.pprint(header_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e5363d02-12c3-47c5-9072-e98901b56163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gas particles contain the following fields: <KeysViewHDF5 ['ArtificialViscosity', 'Coordinates', 'Density', 'ElectronAbundance', 'InternalEnergy', 'Masses', 'Metallicity', 'NeutralHydrogenAbundance', 'ParticleIDs', 'SmoothingLength', 'StarFormationRate', 'Velocities']>\n"
     ]
    }
   ],
   "source": [
    "# now we have learned how to read the metadata (the header) from the HDF5 file\n",
    "# lets demonstrate how to read a given particle type dataset information\n",
    "gas_particles_group = test_file['PartType0']\n",
    "# now we can print what all information is stored for gas particles by simply examining \n",
    "# the keys of the gas_particles_group\n",
    "print(f'Gas particles contain the following fields: {gas_particles_group.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a228c9c-4240-44cf-956b-afea2abfe5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM particles contain the following fields: <KeysViewHDF5 ['Coordinates', 'ParticleIDs', 'Velocities']>\n",
      "\n",
      "Star particles contain the following fields: <KeysViewHDF5 ['Coordinates', 'Masses', 'Metallicity', 'ParticleIDs', 'StellarFormationTime', 'Velocities']>\n"
     ]
    }
   ],
   "source": [
    "# now we can do the same as above for dark matter and stellar particles\n",
    "dm_particles_grp = test_file['PartType1']\n",
    "star_particles_grp = test_file['PartType4']\n",
    "print(f'DM particles contain the following fields: {dm_particles_grp.keys()}\\n')\n",
    "print(f'Star particles contain the following fields: {star_particles_grp.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fdad8188-d2b5-4363-96c2-b9897e2130fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29523.844 34675.754 25684.854]\n",
      " [29520.941 34673.055 25667.387]\n",
      " [29501.701 34667.36  25667.828]\n",
      " ...\n",
      " [32799.797 30466.469 21883.357]\n",
      " [32799.047 30468.588 21883.162]\n",
      " [32798.85  30468.422 21887.209]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# to load the coordinates of gas particles we can do the following now\n",
    "gas_coordinates = gas_particles_group['Coordinates'][:]\n",
    "print(gas_coordinates)\n",
    "print(type(gas_coordinates))\n",
    "# this is a numpy.ndarray of dimensions number_of_gas_particles_in_this_file*3 (x, y, z coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6b7b3168-1298-44d2-90fc-dca3d5e9e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29508.812 34584.164 25751.266]\n",
      " [29512.236 34587.633 25697.383]\n",
      " [29306.955 34558.953 25545.094]\n",
      " ...\n",
      " [32799.137 30467.658 21889.268]\n",
      " [32798.895 30467.377 21889.062]\n",
      " [32799.203 30467.824 21888.955]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# similarly we can load other things, if not sure about the dimensions, be sure to check them out individually\n",
    "# load dark matter particles coordinates\n",
    "dm_coordinates = dm_particles_grp['Coordinates'][:]\n",
    "print(dm_coordinates)\n",
    "print(type(dm_coordinates))\n",
    "# same as above, it is a number_of_dm_particles_in_this_file*3 (x, y, z coordinates)\n",
    "# if we want to just load the x-positions of the dm particles, we can also just do \n",
    "dm_x_coordinates = dm_particles_grp['Coordinates'][:, 0]\n",
    "# but when possible try to load and work with ndarrays as they are fast and efficient and the recommended way \n",
    "# to do things in numpy. ndarrays allow for vectorization, which is super useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0acb877-0666-4d6c-ae47-bc635825f57e",
   "metadata": {},
   "source": [
    "**Similarly you can read any other type of information that is present for an individual group in the HDF5 file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b5fd2-7a6b-4707-a9a8-7389596b8e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
